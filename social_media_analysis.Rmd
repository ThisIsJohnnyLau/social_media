---
title: "Social Media Analysis"
output:
  html_document:
    output_dir: "html"
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    # css: ../../../styles.css
  pdf_document: default

---

# Understanding Twitter data

This is an introduction to the power of Twitter data and what you can achieve using social media analysis.

The first step is accessing tweets via the ***Twitter API*** and then leveraging the power of the `rtweet` libary.  Next we can use this extracted data for social media analysis.
  
  
## Power of twitter data

The volume and velocity of tweets posted on twitter every second is an indicator of the power of twitter data.

The enormous amount of information available, from the tweet text and its metadata, gives great scope for analyzing extracted tweets and deriving insights.

The following code extracts a 1% random sample of live tweets using `stream_tweets()` for a 10 second window and saves it into a data frame.

The dimensions of the data frame will give insights about the number of live tweets extracted and the number of columns that contain the actual tweeted text and metadata on the tweets.


```{r message=FALSE}
# Load libraries
library(rtweet)
library(httpuv)
library(tidyverse)
```


```{r}
# # Extract live tweets for 10 seconds window
# tweets30s <- stream_tweets("", timeout = 30)
# 
# # View dimensions of the data frame with live tweets
# dim(tweets10s)
```

[You can see that a 1% sample of tweets, streamed in 10 seconds, has generated 6878 tweets and 90 columns of information on the tweets. Let's explore and analyze this rich information in the forthcoming lessons.

Twitter allows the extraction of only a limited number of tweets with a free account.]

## Search and extract tweets

Many functions are available in R to extract twitter data for analysis.

`search_tweets()` is a powerful function from rtweet which is used to extract tweets based on a search query.

The function returns a maximum of 18,000 tweets for each request posted.


```{r}
# Extract tweets on "#TedXGlasgow" and include retweets    
twts_tedx <- search_tweets("#TedXGlasgow", 
                 n = 18000, 
                 include_rts = TRUE, 
                 lang = "en")

# View output for the first 5 columns and 10 rows
head(twts_tedx[,1:5], 10)
  


# Extract tweets on "TEDxGlaClimate" and include retweets    
twts_tedx_climate <- search_tweets("TEDxGlaClimate", 
                 n = 18000, 
                 include_rts = TRUE, 
                 lang = "en")

# View output for the first 5 columns and 10 rows
head(twts_tedx[,1:5], 10)
```

You can see various tweets posted by users.

## Search and extract timelines

Similar to `search_tweets()`, `get_timeline()` is another function in the `rtweet` library that can be used to extract tweets.

The `get_timeline()` function is different from `search_tweets()`. It extracts tweets posted by a given user to their timeline instead of searching based on a query.

The `get_timeline()` function can extract upto 3200 tweets at a time.

Tweets posted by TedXGlasgow are extracted below.

```{r}
# Extract tweets posted by the user @TedXGlasgow
get_TedX <- get_timeline("@TedXGlasgow", n = 3200)

# View output
print(get_TedX)
```

***Comment***  


## User interest and tweet counts

The metadata components of extracted twitter data can be analyzed to derive insights.

To identify twitter users who are interested in a topic, you can look at users who tweet often on that topic. The insights derived can be used to promote targeted events to interested users.

The code below identifies users who have tweeted often on the topic "TedXGlasgow".


```{r}
# Create a table of users and tweet counts for the topic
sc_name <- table(get_TedX$screen_name)

# Sort the table in descending order of tweet counts
sc_name_sort <- sort(sc_name, decreasing = TRUE)

# View sorted table for top 10 users
head(sc_name_sort, 10)
```

***Comment***  



## Compare follower count

The follower count for a twitter account indicates the popularity of the personality or a business entity and is a measure of influence in social media.

Knowing the follower counts helps digital marketers strategically position ads on popular twitter accounts for increased visibility.

The follow code extracts user data and compare followers count for twitter accounts of popular Scottish news sites.

```{r}
# Extract user data for the twitter accounts of news sites and Darin O Lien for comparison
users <- lookup_users(c("DarinOlien", "ScotEntNews", "BBCScotlandNews", "STVNews", "heraldscotland", "Scotland", "VisitScotNews", "TheScotsman", "BBCRadioScot"))

# Create a data frame of screen names and follower counts
user_df <- users[,c("screen_name","followers_count")]

# Display and compare the follower counts for the 4 news sites
user_df
```

***Comment***  

## Retweet counts

A retweet helps utilize existing content to build a following for your brand.

The number of times a twitter text is retweeted indicates what is trending. The inputs gathered can be leveraged by promoting your brand using the popular retweets.

The code below identifies tweets on "TEDxGlasClimate" that have been retweeted the most.

```{r}
# Create a data frame of tweet text and retweet count
rtwt <- twts_tedx_climate[,c("text", "retweet_count")]
head(rtwt)

# Sort data frame based on descending order of retweet counts
rtwt_sort <- arrange(rtwt, desc(retweet_count))

# Exclude rows with duplicate text from sorted data frame
rtwt_unique <- unique(rtwt_sort, by = "text")

# Print top 6 unique posts retweeted most number of times
rownames(rtwt_unique) <- NULL
head(rtwt_unique)
```

You have learned use cases of key components present in extracted twitter data. The output shows tweets on Artificial Intelligence that were retweeted the most. Let's continue our learning with the next chapter.

# Analyzing Twitter data
  
It’s time to go deeper. Learn how you can apply filters to tweets and analyze Twitter user data using the golden ratio and the Twitter lists they subscribe to. You’ll also learn how to extract trending topics and analyze Twitter data over time to identify interesting insights.

## Filtering for original tweets

An original tweet is an original posting by a twitter user and is not a retweet, quote, or reply.

The "-filter" can be combined with a search query to exclude retweets, quotes, and replies during tweet extraction.

In this exercise, you will extract tweets on "Superbowl" that are original posts and not retweets, quotes, or replies.

The libraries rtweet and plyr have been pre-loaded for this exercise.

Instructions
100 XP
Extract tweets that are not retweets, quotes, or replies.
Check for the presence of replies.
Check for the presence of quotes.
Check for the presence of retweets.

```{r}
# Extract 5000 original tweets on "Climate"
tweets_org <- search_tweets("Climate -filter:retweets -filter:quote -filter:replies", n = 5000)

# Check for presence of replies
tweets_org %>% 
    count(reply_to_screen_name)
        
# Check for presence of quotes
tweets_org %>% 
    count(is_quote)

# Check for presence of retweets
tweets_org %>% 
    count(is_retweet)
```

For (just shy of) the 5000 tweets, the output of NA for reply_to_screen_name and FALSE for is_quote and is_retweets confirms that the filtered tweets are original posts and not replies, quotes, or retweets.

## Filtering on tweet language

You can use the language filter with a search query to filter tweets based on the language of the tweet.

The filter extracts tweets that have been classified by Twitter as being of a particular language.

Can you extract tweets posted in French on the topic "Apple iphone"?

The library rtweet has been pre-loaded for this exercise.


```{r}
# Extract tweets on "Climate" in French
tweets_french <- search_tweets("Climate", lang = "fr")

# Display the tweets
head(tweets_french$text)

# Display the tweet metadata showing the language
head(tweets_french$lang)
```

## Filter based on tweet popularity

Popular tweets are tweets that are retweeted and favorited several times.

They are useful in identifying current trends. A brand can promote its merchandise and build brand loyalty by identifying popular tweets and retweeting them.

In this exercise, you will extract tweets on "TEDx" that have been retweeted a minimum of 100 times and also favorited at least by 100 users.


```{r}
# Extract tweets with a minimum of 100 retweets and 100 favorites
tweets_pop <- search_tweets("TEDx min_retweets:100 AND min_faves:100")

# Create a data frame to check retweet and favorite counts
counts <- tweets_pop[c("retweet_count", "favorite_count")]
head(counts)

# View the tweets
head(tweets_pop$text)
```

You can see that the extracted tweets received a minimum of 100 retweets and 100 favorites. You can also change the minimum value for retweets and favorites from 100 to a higher number if required.

## Extract user information

Analyzing twitter user data provides vital information which can be used to plan relevant promotional strategies.

User information contains data on the number of followers and friends of the twitter user.

The user information may have multiple instances of the same user as the user might have tweeted multiple times on a given subject. You need to take the mean values of the follower and friend counts in order to consider only one instance.

In this exercise, you will extract the number of friends and followers of users who tweet on 

***#TEDxGlasgow related ***

```{r}
# Extract user information of people who have tweeted on the TEDxGlasgow
user_cos <- users_data(twts_tedx)

# View few rows of user data
head(user_cos)

# Aggregate screen name, follower and friend counts
counts_df <- user_cos %>%
               group_by(screen_name) %>%
               summarise(follower = mean(followers_count),
                   friend = mean(friends_count))

# View the output
head(counts_df)
```


The screen names have been tabulated with their corresponding counts of followers and friends. In the next exercise, you will learn how to use this data to calculate the golden ratio.

## Explore users based on the golden ratio

The ratio of the number of followers to the number of friends a user has is called the golden ratio.

This ratio is a useful metric for marketers to strategize promotions.

In this exercise, you will calculate the golden ratio for the aggregated data frame counts_df that was created in the last step of the previous exercise.

The data frame counts_df and library dplyr have been pre-loaded for this exercise.

```{r}
# Calculate and store the golden ratio
counts_df$ratio <- counts_df$follower/counts_df$friend

# Sort the data frame in decreasing order of follower count
counts_sort <- arrange(counts_df, desc(follower))

# View the first few rows
head(counts_sort)

# Select rows where the follower count is greater than 50000
counts_sort[counts_sort$follower>50000,]

# Select rows where the follower count is less than 1000
counts_sort[counts_sort$follower<1000,]
```

You can see that many users having a high follower count also have a high positive ratio. These users can be used as a medium to promote a cosmetic brand to a wide audience.


## Subscribers to twitter lists

A twitter list is a curated group of twitter accounts.

Twitter users subscribe to lists that interest them. Collecting user information from twitter lists could help brands promote products to interested customers.

In this exercise, you will extract lists of the twitter account of "TEDxGlasgow".

```{r}
# library(tidyverse)
# 
# # Extract all the lists "TEDx" subscribes to and view the first 4 columns
# lst_TEDx <- lists_users("TEDxGlasgow")
# 
# lst_TEDx %>% 
#     arrange(desc(subscriber_count)) %>% 
#   head()
# 
# # Extract subscribers of the list "TEDx" and view the first 4 columns
# list_TED_sub <- lists_subscribers("9783131", n = 500) %>% 
#     arrange(followers_count)
# 
# list_TED_sub[,1:4]
# 
# # Create a list of top screen names from the subscribers list
# users <- list_TED_sub$screen_name %>% 
#     head()
# 
# # Extract user information for the list and view the first 4 columns
# users_TEDx_sub <- lookup_users(users)
# users_TEDx_sub
```


You now have extracted user data of potential customers to whom you can promote TEDxGlasgow.

## Trends by country name

Location-specific trends identify popular topics trending in a specific location. You can extract trends at the country level or city level.

It is more meaningful to extract trends around a specific region, in order to focus on twitter audience in that region for targeted marketing of a brand.

Can you extract topics trending in UK and view the trends?


```{r}
# Get topics trending in UK
gt_country <- get_trends("United Kingdom") %>% 
    arrange(desc(tweet_volume)) %>% 
    view()

```

## Trends by city and most tweeted trends

It is meaningful to extract trends around a specific region to focus on twitter audience in that region.

Trending topics in a city provide a chance to promote region-specific events or products.

This code extracts topics that are trending in Glasgow and also look at the most tweeted trends.

Note: tweet_volume is returned for trends only if this data is available.


```{r}
# Get topics trending in Glasgow
gt_city <- get_trends("Glasgow")

# View the first 6 columns
head(gt_city[,1:6])

# Aggregate the trends and tweet volumes
trend_df <- gt_city %>%
    group_by(trend) %>%
    summarise(tweet_vol = mean(tweet_volume))

# Sort data frame on descending order of tweet volumes and print header
trend_df_sort <- arrange(trend_df, desc(tweet_vol))
head(trend_df_sort,10)
```

The most-tweeted trend in Glasgow was 'yeonjun' a Kpop star! This trend considered when promoting a music-related event.  The latest print is #Gray (because of Eastenders??)




## Visualizing frequency of tweets

Visualizing the frequency of tweets over time helps understand the interest level over a product.

It would be interesting to check the interest level and recall for ClimateCrisis by visualizing the frequency of tweets.


```{r}
# Extract tweets on #ClimateCrisis and exclude retweets
ClimateCrisis_twts <- search_tweets("#ClimateCrisis", n = 18000, include_rts = FALSE)

# View the output
head(ClimateCrisis_twts)

# Create a time series plot
ts_plot(ClimateCrisis_twts, by = "hours", color = "blue")
```

***Comment***  
ClimateCrisis appears to have a cyclical pattern over the days available - with lowering peaks most recently.

## Create time series objects

A time series object contains the aggregated frequency of tweets over a specified time interval.

Creating time series objects is the first step before visualizing tweet frequencies for comparison.

This code creates time series objects for two TED events for comparison



```{r}

# Create a time series object for TEDxGlasgow at hourly intervals
TEDxGlasgow_ts <- ts_data(twts_tedx, by ='hours')

# Rename the two columns in the time series object
names(TEDxGlasgow_ts) <- c("time", "TEDxGlasgow_n")

# View datax
TEDxGlasgow_ts %>% 
    view()
```

```{r}
# Create a time series object for TEDxGlaClimate at hourly intervals
TEDxGlaClimate_ts <- ts_data(twts_tedx_climate, by ='hours')

# Rename the two columns in the time series object
names(TEDxGlaClimate_ts) <- c("time", "TEDxGlasClimate_n")

# View datax
TEDxGlaClimate_ts %>% 
    view()
```


```{r}
# Get TEDsummit2019 data for comparison
twts_tedsummit2019 <- search_tweets("#TEDSummit2019", 
                 n = 18000, 
                 include_rts = TRUE, 
                 lang = "en")


# Create a time series object for TEDxEdinburgh at hourly intervals
tedsummit2019_ts <- ts_data(twts_tedsummit2019, by ='hours')

# Rename the two columns in the time series object
names(tedsummit2019_ts) <- c("time", "edin_n") 

# View data
tedsummit2019_ts %>% 
    view()
```

Time series objects aggregate tweet frequencies over time. They are useful for creating time series plots for comparison.

```{r}

```


## Compare tweet frequencies for two tags

The volume of tweets posted for a product is a strong indicator of its "brand" salience. Let's compare brand salience for two competing tags, TEDxGlaClimate and ClimateCrisis.

In the previous exercise, you had created time series objects for tweets on Puma and Nike. You will merge the time series objects and create time series plots to compare the frequency of tweets.

The time series objects for Puma and Nike have been pre-loaded as TEDxGlaClimate and ClimateCrisis respectively.

The libraries rtweet, reshape, and ggplot2 have also been pre-loaded.

```{r}
library(reshape)

# Merge the two time series objects and retain "time" column
merged_df <- merge(TEDxGlasgow_ts, TEDxGlaClimate_ts, by = "time", all = TRUE)
head(merged_df)

# Stack the tweet frequency columns
melt_df <- melt(merged_df, na.rm = TRUE, id.vars = "time")

# View the output
head(melt_df)

# Plot frequency of tweets on Puma and Nike
ggplot(data = melt_df, aes(x = time, y = value, col = variable))+
  geom_line(lwd = 0.8)
```

***Comments***  
For the data available, TEDxGlasClimate seems to have different activity from the main Twitter handle; apart from the peak around Oct 10th.


# Visualize Tweet texts

A picture is worth a thousand words! This following code explores how you can visualize text from tweets using bar plots and word clouds. Tweet text will be processed to prepare a clean text corpus for analysis. Imagine being able to extract key discussion topics and people's perceptions about a subject or brand from the tweets they are sharing. This is possible using topic modeling and sentiment analysis.

## Remove URLs and characters other than letters

Tweet text posted by twitter users is unstructured, noisy, and raw.

It contains emoticons, URLs, and numbers. This redundant information has to be cleaned before analysis in order to yield reliable results.

The code below removes URLs and replaces characters other than letters with spaces.


```{r}
# Loading Regex library
library(qdapRegex)

# Extract tweet text from ClimateCrisis dataset
twt_txt <- twts_tedx_climate$text
head(twt_txt)

# Remove URLs from the tweet text and view the output
twt_txt_url <- rm_twitter_url(twt_txt)
head(twt_txt_url)

# Replace special characters, punctuation, & numbers with spaces
twt_txt_chrs  <- gsub("[^A-Za-z]"," " , twt_txt_url)

# View text after replacing special characters, punctuation, & numbers
head(twt_txt_chrs)
```

The URLs have been removed and special characters, punctuation, & numbers have been replaced with additional spaces in the text.

## Build a corpus and convert to lowercase

A corpus is a list of text documents. You have to convert the tweet text into a corpus to facilitate subsequent steps in text processing.

When analyzing text, you want to ensure that a word is not counted as two different words because the case is different in the two instances. Hence, you need to convert text to lowercase.

The code will create a text corpus and convert all characters to lower case.


```{r}
# Loading text mining library
library(tm)

# Convert text in "twt_gsub" dataset to a text corpus and view output
twt_corpus <- twt_txt_chrs %>% 
                VectorSource() %>% 
                Corpus() 
head(twt_corpus$content)

# Convert the corpus to lowercase
twt_corpus_lwr <- tm_map(twt_corpus, tolower) 

# View the corpus after converting to lowercase
head(twt_corpus_lwr$content)
```

The corpus has been built from the tweet text and converted the characters in the corpus to lowercase.



## Remove stop words and additional spaces

The text corpus usually has many common words like a, an, the, of, and but. These are called stop words.

Stop words are usually removed during text processing so one can focus on the important words in the corpus to derive insights.

Also, the additional spaces created during the removal of special characters, punctuation, numbers, and stop words need to be removed from the corpus.


```{r}
# Remove English stop words from the corpus and view the corpus
twt_corpus_stpwd <- tm_map(twt_corpus_lwr, removeWords, stopwords("english"))
head(twt_corpus_stpwd$content)

# Remove additional spaces from the corpus
twt_corpus_final <- tm_map(twt_corpus_stpwd, stripWhitespace)

# View the text corpus after removing spaces
head(twt_corpus_final$content)
```

You can see some of the common stop words and all the additional spaces removed in the output.

## Removing custom stop words

Popular terms in a text corpus can be visualized using bar plots or word clouds.

However, it is important to remove custom stop words present in the corpus first before using the visualization tools.

The code below will check the term frequencies and remove custom stop words from the text corpus created for "ClimateCrisis".



```{r}
# Loading library for text analysis
library(qdap)

# Extract term frequencies for top 60 words and view output
termfreq  <-  freq_terms(twt_corpus, 60)
termfreq

# Create a vector of custom stop words
# custom_stopwds <- c("s", "amp", "can", "new", "medical", 
# 				"will", "via", "way",  "today", "come", "t", "ways", 
#                 "say", "ai", "get", "now")

# Remove custom stop words and create a refined corpus
corp_refined <- tm_map(twt_corpus,removeWords, stopwords("english")) 

# Extract term frequencies for the top 20 words
termfreq_clean <- freq_terms(corp_refined, 20)
termfreq_clean
```

You can see that the corpus has only the relevant and important terms after the stop words are removed. Let's use this refined corpus to create visualizations in the next two exercises.

## Visualize popular terms with bar plots

Bar plot is a simple yet popular tool used in data visualization.

It quickly helps summarize categories and their values in a visual form.

The code below will create bar plots for the popular terms appearing in a text corpus.


```{r}
# Extract term frequencies for the top 25 words
termfreq_25w <- freq_terms(corp_refined, 25)
termfreq_25w

# Identify terms with more than 50 counts from the top 25 list
term50 <- subset(termfreq_25w, FREQ > 50)
term50

# Create a bar plot using terms with more than 50 counts
ggplot(term50, aes(x = reorder(WORD, -FREQ), y = FREQ)) +
		geom_bar(stat = "identity", fill = "blue") + 
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Terms like climate, join, watch, party are popular.  Bar plots quickly help summarize these popular terms in an easily interpretable form.

## Word clouds for visualization

A word cloud is an image made up of words in which the size of each word indicates its frequency.

It is an effective promotional image for marketing campaigns.

The code below will create word clouds using the words in a text corpus.


```{r}
library(RColorBrewer)
library(wordcloud)

# Create word cloud with 6 colors and max 50 words
wordcloud(corp_refined, max.words = 50, 
    colors = brewer.pal(6, "Dark2"), 
    scale=c(4,1), random.order = FALSE)
```

You can see that popular terms like climate and wedonthavetime are in large font sizes and positioned at the center of the word cloud to highlight their relevance and importance.

## The LDA algorithm

The document term matrix and the number of topics are the inputs for the LDA() function in R.

## Create a document term matrix

The document term matrix or DTM is a matrix representation of a corpus.

Creating the DTM from the text corpus is the first step towards building a topic model.

```{r}
# Create a document term matrix (DTM) for TEDxGlaClimate
dtm_TEDxGlaClimate <- DocumentTermMatrix(corp_refined)
dtm_TEDxGlaClimate

# Find the sum of word counts in each document
rowTotals <- apply(dtm_TEDxGlaClimate, 1, sum)
head(rowTotals)

# Select rows with a row total greater than zero
dtm_TEDxGlaClimate_new <- dtm_TEDxGlaClimate[rowTotals > 0, ]
dtm_TEDxGlaClimate_new
```

You can see that the final DTM has 233 documents and 355 terms. The code below will use this DTM to perform topic modeling

## Create a topic model

Topic modeling is the task of automatically discovering topics from a vast amount of text.

You can create topic models from the tweet text to quickly summarize the vast information available into distinct topics and gain insights.

The code below will extract distinct topics from tweets on "".



```{r}
# Load libraries
library(topicmodels)
```


```{r}
# Create a topic model with 5 topics
topicmodl_5 <- LDA(dtm_TEDxGlaClimate_new, k = 5)

# Select and view the top 10 terms in the topic model
top_10terms <- terms(topicmodl_5, 10)
top_10terms 

```

***Comment***  
sustainability, carbon, energy, water, and global warming aren't in the top topics.  The process can be repeated for ClimateCrisis for comparison.

## Extract sentiment scores

Sentiment analysis is useful in social media monitoring since it gives an overview of people's sentiments.

Climate change is a widely discussed topic for which the perceptions range from being a severe threat to nothing but a hoax.

In this exercise, you will perform sentiment analysis and extract the sentiment scores for tweets on "Climate change".

These cam be used to plot and analyze how the collective sentiment varies among people.


```{r}
library(syuzhet)

# Perform sentiment analysis for tweets on `ClimateCrisis` 
sa.value <- get_nrc_sentiment(ClimateCrisis_twts$text)

# View the sentiment scores
head(sa.value, 10)
```

Next, these scores are used to plot the sentiments and analyze the results.

## Perform sentiment analysis

You have extracted the sentiment scores for tweets on "Climate change" in the previous exercise.

Can we plot and analyze the most prevalent sentiments among people and see how the collective sentiment varies?

```{r}
# Calculate sum of sentiment scores
score <- colSums(sa.value[,])

# Convert the sum of scores to a data frame
score_df <- data.frame(score)

# Convert row names into 'sentiment' column and combine with sentiment scores
score_df2 <- cbind(sentiment = row.names(score_df),  
				  score_df, row.names = NULL)
print(score_df2)

# Plot the sentiment scores
ggplot(data = score_df2, aes(x = sentiment, y = score, fill = sentiment)) +
  	 geom_bar(stat = "identity") +
       theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

It is interesting to see that positive sentiments collectively outnumber the negative ones.  Trust, anticipation and fear' are notable too.

# Network analysis

Twitter users tweet, like, follow, and retweet creating complex network structures. Finally, we analyze these network structures and visualize the relationships between these individual people as a retweet network. By extracting geolocation data from the tweets we’ll also discover how to display tweet locations on a map, and answer powerful questions such as which states or countries are talking about your brand the most? Geographic data adds a new dimension to the Twitter data analysis.



## Preparing data for a retweet network

A retweet network is a network of twitter users who retweet tweets posted by other users.

People who retweet on travel can be potential players for broadcasting messages of a travel portal.

The following code will prepare the tweet data on #travel for creating a retweet network.


```{r}
# Extract source vertex and target vertex from the tweet data frame
rtwt_df <- twts_tedx_climate[, c("screen_name" , "retweet_screen_name" )]

# View the data frame
head(rtwt_df)

# Remove rows with missing values
rtwt_df_new <- rtwt_df[complete.cases(rtwt_df), ]

# Create a matrix
rtwt_matrx <- as.matrix(rtwt_df_new)
head(rtwt_matrx)
```

## Create a retweet network

The core step in network analysis is to create a network object like a retweet network as it helps analyze the inter-relationships between the nodes.

Understanding the position of potential customers on a retweet network allows a brand to identify key players who are likely to retweet posts to spread brand messaging.

Can you create a retweet network on #travel using the matrix saved in the previous exercise?

The matrix rtwt_matrx and the library igraph have been pre-loaded for this exercise.

